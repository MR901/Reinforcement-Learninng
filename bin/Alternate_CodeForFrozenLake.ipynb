{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# basic_rl.py (v0.0.1)\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Use SARSA/Q-learning algorithm with epsilon-greedy/softmax polciy.')\n",
    "parser.add_argument('-a', '--algorithm', default='sarsa', choices=['sarsa', 'q_learning'],\n",
    "                    help=\"Type of learning algorithm. (Default: sarsa)\")\n",
    "parser.add_argument('-p', '--policy', default='epsilon_greedy', choices=['epsilon_greedy', 'softmax'],\n",
    "                    help=\"Type of policy. (Default: epsilon_greedy)\")\n",
    "parser.add_argument('-e', '--environment', default='FrozenLake-v0',\n",
    "                    help=\"Name of the environment provided in the OpenAI Gym. (Default: FrozenLake-v0)\")\n",
    "parser.add_argument('-n', '--nepisode', default='5000', type=int,\n",
    "                    help=\"Number of episode. (Default: 5000)\")\n",
    "parser.add_argument('-al', '--alpha', default='0.1', type=float,\n",
    "                    help=\"Learning rate. (Default: 0.1)\")\n",
    "parser.add_argument('-be', '--beta', default='0.0', type=float,\n",
    "                    help=\"Initial value of an inverse temperature. (Default: 0.0)\")\n",
    "parser.add_argument('-bi', '--betainc', default='0.02', type=float,\n",
    "                    help=\"Linear increase rate of an inverse temperature. (Default: 0.02)\")\n",
    "parser.add_argument('-ga', '--gamma', default='0.99', type=float,\n",
    "                    help=\"Discount rate. (Default: 0.99)\")\n",
    "parser.add_argument('-ep', '--epsilon', default='0.5', type=float,\n",
    "                    help=\"Fraction of random exploration in the epsilon greedy. (Default: 0.5)\")\n",
    "parser.add_argument('-ed', '--epsilondecay', default='0.999', type=float,\n",
    "                    help=\"Decay rate of epsilon in the epsilon greedy. (Default: 0.999)\")\n",
    "parser.add_argument('-ms', '--maxstep', default='100', type=int,\n",
    "                    help=\"Maximum step allowed in a episode. (Default: 100)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(q_value, beta=1.0):\n",
    "    assert beta >= 0.0\n",
    "    q_tilde = q_value - np.max(q_value)\n",
    "    factors = np.exp(beta * q_tilde)\n",
    "    return factors / np.sum(factors)\n",
    "\n",
    "def select_a_with_softmax(curr_s, q_value, beta=1.0):\n",
    "    prob_a = softmax(q_value[curr_s, :], beta=beta)\n",
    "    cumsum_a = np.cumsum(prob_a)\n",
    "    return np.where(np.random.rand() < cumsum_a)[0][0]\n",
    "\n",
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a\n",
    "\n",
    "def main():\n",
    "\n",
    "    env_type = args.environment\n",
    "    algorithm_type = args.algorithm\n",
    "    policy_type = args.policy\n",
    "\n",
    "    \n",
    "    # Random seed\n",
    "    np.random.RandomState(42)\n",
    "\n",
    "    # Selection of the problem\n",
    "    env = gym.envs.make(env_type)\n",
    "\n",
    "    \n",
    "    \n",
    "    # Constraints imposed by the environment\n",
    "    n_a = env.action_space.n\n",
    "    n_s = env.observation_space.n\n",
    "\n",
    "    # Meta parameters for the RL agent\n",
    "    alpha = args.alpha\n",
    "    beta = init_beta = args.beta\n",
    "    beta_inc = args.betainc\n",
    "    gamma = args.gamma\n",
    "    epsilon = args.epsilon\n",
    "    epsilon_decay = args.epsilondecay\n",
    "\n",
    "    # Experimental setup\n",
    "    n_episode = args.nepisode\n",
    "    print \"n_episode \", n_episode\n",
    "    max_step = args.maxstep\n",
    "\n",
    "    # Initialization of a Q-value table\n",
    "    q_value = np.zeros([n_s, n_a])\n",
    "\n",
    "    # Initialization of a list for storing simulation history\n",
    "    history = []\n",
    "\n",
    "    print \"algorithm_type: {}\".format(algorithm_type)\n",
    "    print \"policy_type: {}\".format(policy_type)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "    result_dir = 'results-{0}-{1}-{2}'.format(env_type, algorithm_type, policy_type)\n",
    "\n",
    "    # Start monitoring the simulation for OpenAI Gym\n",
    "    env.monitor.start(result_dir, force=True)\n",
    "\n",
    "    for i_episode in xrange(n_episode):\n",
    "\n",
    "        # Reset a cumulative reward for this episode\n",
    "        cumu_r = 0\n",
    "\n",
    "        # Start a new episode and sample the initial state\n",
    "        curr_s = env.reset()\n",
    "\n",
    "        # Select the first action in this episode\n",
    "        if policy_type == 'softmax':\n",
    "            curr_a = select_a_with_softmax(curr_s, q_value, beta=beta)\n",
    "        elif policy_type == 'epsilon_greedy':\n",
    "            curr_a = select_a_with_epsilon_greedy(curr_s, q_value, epsilon=epsilon)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "        for i_step in xrange(max_step):\n",
    "\n",
    "            # Get a result of your action from the environment\n",
    "            next_s, r, done, info = env.step(curr_a)\n",
    "\n",
    "            # Modification of reward (not sure if it's OK to change reward setting by hand...)\n",
    "            if done & (r == 0):\n",
    "                # Punishment for falling into a hall\n",
    "                r = 0.0\n",
    "            elif not done:\n",
    "                # Cost per step\n",
    "                r = -0.001\n",
    "\n",
    "            # Update a cummulative reward\n",
    "            cumu_r = r + gamma * cumu_r\n",
    "\n",
    "            # Select an action\n",
    "            if policy_type == 'softmax':\n",
    "                next_a = select_a_with_softmax(next_s, q_value, beta=beta)\n",
    "            elif policy_type == 'epsilon_greedy':\n",
    "                next_a = select_a_with_epsilon_greedy(next_s, q_value, epsilon=epsilon)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid policy_type: {}\".format(policy_type))            \n",
    "\n",
    "            # Calculation of TD error\n",
    "            if algorithm_type == 'sarsa':\n",
    "                delta = r + gamma * q_value[next_s, next_a] - q_value[curr_s, curr_a]\n",
    "            elif algorithm_type == 'q_learning':\n",
    "                delta = r + gamma * np.max(q_value[next_s, :]) - q_value[curr_s, curr_a]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid algorithm_type: {}\".format(algorithm_type))\n",
    "\n",
    "            # Update a Q value table\n",
    "            q_value[curr_s, curr_a] += alpha * delta\n",
    "\n",
    "            curr_s = next_s\n",
    "            curr_a = next_a\n",
    "\n",
    "            if done:\n",
    "                if policy_type == 'softmax':\n",
    "                    print \"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tBeta: {4:.3f}\".format(i_episode, i_step, cumu_r, r, beta)\n",
    "                    history.append([i_episode, i_step, cumu_r, r, beta])\n",
    "                elif policy_type == 'epsilon_greedy':                \n",
    "                    print \"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tEpsilon: {4:.3f}\".format(i_episode, i_step, cumu_r, r, epsilon)\n",
    "                    history.append([i_episode, i_step, cumu_r, r, epsilon])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "                break\n",
    "\n",
    "        if policy_type == 'epsilon_greedy':\n",
    "            # epsilon is decayed expolentially\n",
    "            epsilon = epsilon * epsilon_decay\n",
    "        elif policy_type == 'softmax':\n",
    "            # beta is increased linearly\n",
    "            beta = init_beta + i_episode * beta_inc\n",
    "\n",
    "    # Stop monitoring the simulation for OpenAI Gym\n",
    "    env.monitor.close()\n",
    "\n",
    "    history = np.array(history)\n",
    "\n",
    "    window_size = 100\n",
    "    def running_average(x, window_size, mode='valid'):\n",
    "        return np.convolve(x, np.ones(window_size)/window_size, mode=mode)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=[12, 8])\n",
    "    # Number of steps\n",
    "    ax[0, 0].plot(history[:, 0], history[:, 1], '.') \n",
    "    ax[0, 0].set_xlabel('Episode')\n",
    "    ax[0, 0].set_ylabel('Number of steps')\n",
    "    ax[0, 0].plot(history[window_size-1:, 0], running_average(history[:, 1], window_size))\n",
    "    # Cumulative reward\n",
    "    ax[0, 1].plot(history[:, 0], history[:, 2], '.') \n",
    "    ax[0, 1].set_xlabel('Episode')\n",
    "    ax[0, 1].set_ylabel('Cumulative rewards')\n",
    "    ax[0, 1].plot(history[window_size-1:, 0], running_average(history[:, 2], window_size))\n",
    "    # Terminal reward\n",
    "    ax[1, 0].plot(history[:, 0], history[:, 3], '.') \n",
    "    ax[1, 0].set_xlabel('Episode')\n",
    "    ax[1, 0].set_ylabel('Terminal rewards')\n",
    "    ax[1, 0].plot(history[window_size-1:, 0], running_average(history[:, 3], window_size))\n",
    "    # Epsilon/Beta\n",
    "    ax[1, 1].plot(history[:, 0], history[:, 4], '.') \n",
    "    ax[1, 1].set_xlabel('Episode')\n",
    "    if policy_type == 'softmax':\n",
    "        ax[1, 1].set_ylabel('Beta')\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        ax[1, 1].set_ylabel('Epsilon')\n",
    "    fig.savefig('./'+result_dir+'.png')\n",
    "\n",
    "    print \"Q value table:\"\n",
    "    print q_value\n",
    "\n",
    "    if policy_type == 'softmax':\n",
    "        print \"Action selection probability:\"\n",
    "        print np.array([softmax(q, beta=beta) for q in q_value])\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        print \"Greedy action\"\n",
    "        greedy_action = np.zeros([n_s, n_a])\n",
    "        greedy_action[np.arange(n_s), np.argmax(q_value, axis=1)] = 1\n",
    "        #print np.array([zero_vec[np.argmax(q)] = 1 for q in q_value])\n",
    "        print greedy_action\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# basic_rl.py (v0.0.2)\n",
    "\n",
    "import argparse\n",
    "parser = argparse.ArgumentParser(description='Use SARSA/Q-learning algorithm with epsilon-greedy/softmax polciy.')\n",
    "parser.add_argument('-a', '--algorithm', default='sarsa', choices=['sarsa', 'q_learning'],\n",
    "                    help=\"Type of learning algorithm. (Default: sarsa)\")\n",
    "parser.add_argument('-p', '--policy', default='epsilon_greedy', choices=['epsilon_greedy', 'softmax'],\n",
    "                    help=\"Type of policy. (Default: epsilon_greedy)\")\n",
    "parser.add_argument('-e', '--environment', default='FrozenLake8x8-v0',\n",
    "                    help=\"Name of the environment provided in the OpenAI Gym. (Default: FrozenLake8x8-v0)\")\n",
    "parser.add_argument('-n', '--nepisode', default='20000', type=int,\n",
    "                    help=\"Number of episode. (Default: 20000)\")\n",
    "parser.add_argument('-al', '--alpha', default='0.1', type=float,\n",
    "                    help=\"Learning rate. (Default: 0.1)\")\n",
    "parser.add_argument('-be', '--beta', default='0.0', type=float,\n",
    "                    help=\"Initial value of an inverse temperature. (Default: 0.0)\")\n",
    "parser.add_argument('-bi', '--betainc', default='0.25', type=float,\n",
    "                    help=\"Linear increase rate of an inverse temperature. (Default: 0.25)\")\n",
    "parser.add_argument('-ga', '--gamma', default='0.99', type=float,\n",
    "                    help=\"Discount rate. (Default: 0.99)\")\n",
    "parser.add_argument('-ep', '--epsilon', default='0.7', type=float,\n",
    "                    help=\"Fraction of random exploration in the epsilon greedy. (Default: 0.7)\")\n",
    "parser.add_argument('-ed', '--epsilondecay', default='0.995', type=float,\n",
    "                    help=\"Decay rate of epsilon in the epsilon greedy. (Default: 0.995)\")\n",
    "parser.add_argument('-ms', '--maxstep', default='100', type=int,\n",
    "                    help=\"Maximum step allowed in a episode. (Default: 100)\")\n",
    "parser.add_argument('-ka', '--kappa', default='0.01', type=float,\n",
    "                    help=\"Weight of the most recent terminal reward for computing its running average. (Default: 0.01)\")\n",
    "args = parser.parse_args()\n",
    "\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def softmax(q_value, beta=1.0):\n",
    "    assert beta >= 0.0\n",
    "    q_tilde = q_value - np.max(q_value)\n",
    "    factors = np.exp(beta * q_tilde)\n",
    "    return factors / np.sum(factors)\n",
    "\n",
    "def select_a_with_softmax(curr_s, q_value, beta=1.0):\n",
    "    prob_a = softmax(q_value[curr_s, :], beta=beta)\n",
    "    cumsum_a = np.cumsum(prob_a)\n",
    "    return np.where(np.random.rand() < cumsum_a)[0][0]\n",
    "\n",
    "def select_a_with_epsilon_greedy(curr_s, q_value, epsilon=0.1):\n",
    "    a = np.argmax(q_value[curr_s, :])\n",
    "    if np.random.rand() < epsilon:\n",
    "        a = np.random.randint(q_value.shape[1])\n",
    "    return a\n",
    "\n",
    "def main():\n",
    "\n",
    "    env_type = args.environment\n",
    "    algorithm_type = args.algorithm\n",
    "    policy_type = args.policy\n",
    "    \n",
    "    # Random seed\n",
    "    np.random.RandomState(42)\n",
    "\n",
    "    # Selection of the problem\n",
    "    env = gym.envs.make(env_type)\n",
    "    \n",
    "    # Constraints imposed by the environment\n",
    "    n_a = env.action_space.n\n",
    "    n_s = env.observation_space.n\n",
    "\n",
    "    # Meta parameters for the RL agent\n",
    "    alpha = args.alpha\n",
    "    beta = args.beta\n",
    "    beta_inc = args.betainc\n",
    "    gamma = args.gamma\n",
    "    epsilon = args.epsilon\n",
    "    epsilon_decay = args.epsilondecay\n",
    "\n",
    "    # Experimental setup\n",
    "    n_episode = args.nepisode\n",
    "    print \"n_episode \", n_episode\n",
    "    max_step = args.maxstep\n",
    "\n",
    "    # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "    # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "    # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "    kappa = args.kappa\n",
    "    ave_terminal_r = None\n",
    "    \n",
    "    # Initialization of a Q-value table\n",
    "    q_value = np.zeros([n_s, n_a])\n",
    "\n",
    "    # Initialization of a list for storing simulation history\n",
    "    history = []\n",
    "    \n",
    "    print \"algorithm_type: {}\".format(algorithm_type)\n",
    "    print \"policy_type: {}\".format(policy_type)\n",
    "\n",
    "    env.reset()\n",
    "\n",
    "    np.set_printoptions(precision=3, suppress=True)\n",
    "\n",
    "    result_dir = 'results-{0}-{1}-{2}'.format(env_type, algorithm_type, policy_type)\n",
    "\n",
    "    # Start monitoring the simulation for OpenAI Gym\n",
    "    env.monitor.start(result_dir, force=True)\n",
    "\n",
    "    for i_episode in xrange(n_episode):\n",
    "\n",
    "        # Reset a cumulative reward for this episode\n",
    "        cumu_r = 0\n",
    "\n",
    "        # Start a new episode and sample the initial state\n",
    "        curr_s = env.reset()\n",
    "\n",
    "        # Select the first action in this episode\n",
    "        if policy_type == 'softmax':\n",
    "            curr_a = select_a_with_softmax(curr_s, q_value, beta=beta)\n",
    "        elif policy_type == 'epsilon_greedy':\n",
    "            curr_a = select_a_with_epsilon_greedy(curr_s, q_value, epsilon=epsilon)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "        for i_step in xrange(max_step):\n",
    "\n",
    "            # Get a result of your action from the environment\n",
    "            next_s, r, done, info = env.step(curr_a)\n",
    "\n",
    "            # Modification of reward\n",
    "            # CAUTION: Changing this part of the code in order to get a fast convergence\n",
    "            # is not a good idea because it is essentially changing the problem setting itself.\n",
    "            # This part of code was kept not to get fast convergence but to show the \n",
    "            # influence of a reward function on the convergence speed for pedagogical reason.\n",
    "            #if done & (r == 0):\n",
    "            #    # Punishment for falling into a hall\n",
    "            #    r = 0.0\n",
    "            #elif not done:\n",
    "            #    # Cost per step\n",
    "            #    r = 0.0\n",
    "\n",
    "            # Update a cummulative reward\n",
    "            cumu_r = r + gamma * cumu_r\n",
    "\n",
    "            # Select an action\n",
    "            if policy_type == 'softmax':\n",
    "                next_a = select_a_with_softmax(next_s, q_value, beta=beta)\n",
    "            elif policy_type == 'epsilon_greedy':\n",
    "                next_a = select_a_with_epsilon_greedy(next_s, q_value, epsilon=epsilon)\n",
    "            else:\n",
    "                raise ValueError(\"Invalid policy_type: {}\".format(policy_type))            \n",
    "\n",
    "            # Calculation of TD error\n",
    "            if algorithm_type == 'sarsa':\n",
    "                delta = r + gamma * q_value[next_s, next_a] - q_value[curr_s, curr_a]\n",
    "            elif algorithm_type == 'q_learning':\n",
    "                delta = r + gamma * np.max(q_value[next_s, :]) - q_value[curr_s, curr_a]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid algorithm_type: {}\".format(algorithm_type))\n",
    "\n",
    "            # Update a Q value table\n",
    "            q_value[curr_s, curr_a] += alpha * delta\n",
    "\n",
    "            curr_s = next_s\n",
    "            curr_a = next_a\n",
    "\n",
    "            if done:\n",
    "\n",
    "                # Running average of the terminal reward, which is used for controlling an exploration rate\n",
    "                # (This idea of controlling exploration rate by the terminal reward is suggested by JKCooper2)\n",
    "                # See https://gym.openai.com/evaluations/eval_xSOlwrBsQDqUW7y6lJOevQ\n",
    "                kappa = 0.01\n",
    "                if ave_terminal_r == None:\n",
    "                    ave_terminal_r = r\n",
    "                else:\n",
    "                    ave_terminal_r = kappa * r + (1 - kappa) * ave_terminal_r\n",
    "                \n",
    "                if r > ave_terminal_r:\n",
    "                    # Bias the current policy toward exploitation\n",
    "                    \n",
    "                    if policy_type == 'epsilon_greedy':\n",
    "                        # epsilon is decayed expolentially\n",
    "                        epsilon = epsilon * epsilon_decay\n",
    "                    elif policy_type == 'softmax':\n",
    "                        # beta is increased linearly\n",
    "                        beta = beta + beta_inc\n",
    "                        \n",
    "                if policy_type == 'softmax':\n",
    "                    print \"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tAveTermR: {4:.3f}\\tBeta: {5:.3f}\".format(\n",
    "                        i_episode, i_step, cumu_r, r, ave_terminal_r, beta)\n",
    "                    history.append([i_episode, i_step, cumu_r, r, ave_terminal_r, beta])\n",
    "                elif policy_type == 'epsilon_greedy':\n",
    "                    print \"Episode: {0}\\t Steps: {1:>4}\\tCumuR: {2:>5.2f}\\tTermR: {3}\\tAveTermR: {4:.3f}\\tEpsilon: {5:.3f}\".format(\n",
    "                        i_episode, i_step, cumu_r, r, ave_terminal_r, epsilon)\n",
    "                    history.append([i_episode, i_step, cumu_r, r, ave_terminal_r, epsilon])\n",
    "                else:\n",
    "                    raise ValueError(\"Invalid policy_type: {}\".format(policy_type))\n",
    "\n",
    "                break\n",
    "\n",
    "    # Stop monitoring the simulation for OpenAI Gym\n",
    "    env.monitor.close()\n",
    "\n",
    "    history = np.array(history)\n",
    "\n",
    "    window_size = 100\n",
    "    def running_average(x, window_size, mode='valid'):\n",
    "        return np.convolve(x, np.ones(window_size)/window_size, mode=mode)\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=[12, 8])\n",
    "    # Number of steps\n",
    "    ax[0, 0].plot(history[:, 0], history[:, 1], '.') \n",
    "    ax[0, 0].set_xlabel('Episode')\n",
    "    ax[0, 0].set_ylabel('Number of steps')\n",
    "    ax[0, 0].plot(history[window_size-1:, 0], running_average(history[:, 1], window_size))\n",
    "    # Cumulative reward\n",
    "    ax[0, 1].plot(history[:, 0], history[:, 2], '.') \n",
    "    ax[0, 1].set_xlabel('Episode')\n",
    "    ax[0, 1].set_ylabel('Cumulative rewards')\n",
    "    ax[0, 1].plot(history[window_size-1:, 0], running_average(history[:, 2], window_size))\n",
    "    # Terminal reward\n",
    "    ax[1, 0].plot(history[:, 0], history[:, 3], '.') \n",
    "    ax[1, 0].set_xlabel('Episode')\n",
    "    ax[1, 0].set_ylabel('Terminal rewards')\n",
    "    ax[1, 0].plot(history[window_size-1:, 0], running_average(history[:, 3], window_size))\n",
    "    # Epsilon/Beta\n",
    "    ax[1, 1].plot(history[:, 0], history[:, 5], '.') \n",
    "    ax[1, 1].set_xlabel('Episode')\n",
    "    if policy_type == 'softmax':\n",
    "        ax[1, 1].set_ylabel('Beta')\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        ax[1, 1].set_ylabel('Epsilon')\n",
    "    fig.savefig('./'+result_dir+'.png')\n",
    "\n",
    "    print \"Q value table:\"\n",
    "    print q_value\n",
    "\n",
    "    if policy_type == 'softmax':\n",
    "        print \"Action selection probability:\"\n",
    "        print np.array([softmax(q, beta=beta) for q in q_value])\n",
    "    elif policy_type == 'epsilon_greedy':\n",
    "        print \"Greedy action\"\n",
    "        greedy_action = np.zeros([n_s, n_a])\n",
    "        greedy_action[np.arange(n_s), np.argmax(q_value, axis=1)] = 1\n",
    "        #print np.array([zero_vec[np.argmax(q)] = 1 for q in q_value])\n",
    "        print greedy_action\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProjectVirEnv",
   "language": "python",
   "name": "myprojectvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
