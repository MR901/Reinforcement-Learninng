{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monte Carlo\n",
    "\n",
    "**Any method which solves a problem by generating suitable random numbers, and observing that fraction of numbers obeying some property or properties, can be classified as a Monte Carlo method.**\n",
    "\n",
    "Let’s do a fun exercise where we will try to find out the value of pi using pen and paper. Let’s draw a square of unit length and draw a quarter circle with unit length radius. Now, we have a helper bot C3PO with us. It is tasked with putting as many dots as possible on the square randomly 3,000 times, resulting in the following figure:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "C3PO needs to count each time it puts a dot inside a circle. So, the value of pi will be given by:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "where N is the number of times a dot was put inside the circle. As you can see, we did not do anything except count the random dots that fall inside the circle and then took a ratio to approximate the value of pi.\n",
    "\n",
    "### Monte Carlo Reinforcement Learning\n",
    "The Monte Carlo method for reinforcement learning learns directly from episodes of experience without any prior knowledge of MDP transitions. Here, the random component is the return or reward.\n",
    "\n",
    "One caveat is that it can only be applied to episodic MDPs. Its fair to ask why, at this point. The reason is that the episode has to terminate before we can calculate any returns. Here, we don’t do an update after every action, but rather after every episode. It uses the simplest idea – the value is the mean return of all sample trajectories for each state.\n",
    "\n",
    "Recalling the idea from multi-armed bandits discussed in this article, every state is a separate multi-armed bandit problem and the idea is to behave optimally for all multi-armed bandits at once.\n",
    "\n",
    "Similar to dynamic programming, there is a policy evaluation (finding the value function for a given random policy) and policy improvement step (finding the optimum policy). We will cover both these steps in the next two sections.\n",
    "\n",
    " \n",
    "\n",
    "### Monte Carlo Policy Evaluation\n",
    "The goal here, again, is to learn the value function vpi(s) from episodes of experience under a policy pi. Recall that the return is the total discounted reward:\n",
    "\n",
    "$$S_{1}, A_{1}, R_{2}, ….S_{k} ~ pi$$\n",
    "\n",
    "Also recall that the value function is the expected return:\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "\n",
    "We know that we can estimate any expected value simply by adding up samples and dividing by the total number of samples:\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "\n",
    "i – Episode index\n",
    "s – Index of state\n",
    "The question is how do we get these sample returns? For that, we need to play a bunch of episodes and generate them.\n",
    "\n",
    "For every episode we play, we’ll have a sequence of states and rewards. And from these rewards, we can calculate the return by definition, which is just the sum of all future rewards.\n",
    "\n",
    "**First Visit Monte Carlo:** Average returns only for first time s is visited in an episode.\n",
    "\n",
    "Here’s a step-by-step view of how the algorithm works:\n",
    "\n",
    "1. Initialize the policy, state-value function\n",
    "2. Start by generating an episode according to the current policy\n",
    "    1. Keep track of the states encountered through that episode\n",
    "3. Select a state in 2.1\n",
    "    1. Add to a list the return received after first occurrence of this state\n",
    "    2. Average over all returns\n",
    "    3. Set the value of the state as that computed average\n",
    "4. Repeat step 3\n",
    "5. Repeat 2-4 until satisfied\n",
    "**Every visit Monte Carlo:** Average returns for every time s is visited in an episode.\n",
    "\n",
    "For this algorithm, we just change step #3.1 to ‘Add to a list the return received after every occurrence of this state’.\n",
    "\n",
    "Let’s consider a simple example to further understand this concept. Suppose there’s an environment where we have 2 states – A and B. Let’s say we observed 2 sample episodes:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "A+3 => A indicates a transition from state A to state A, with a reward +3. Let’s find out the value function using both methods:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "\n",
    "### Incremental Mean\n",
    "It is convenient to convert the mean return into an incremental update so that the mean can be updated with each episode and we can understand the progress made with each episode. We already learnt this when solving the multi-armed bandit problem.\n",
    "\n",
    "We update v(s) incrementally after episodes. For each state St, with return Gt:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "In non-stationary problems, it can be useful to track a running mean, i.e., forget old episodes:\n",
    "\n",
    "$$V(S_{t}) ← V(S_{t}) + α (G_{t} − V(S_{t}))$$\n",
    "\n",
    " \n",
    "\n",
    "### Monte Carlo Control\n",
    "Similar to dynamic programming, once we have the value function for a random policy, the important task that still remains is that of finding the optimal policy using Monte Carlo.\n",
    "\n",
    "Recall that the formula for policy improvement in DP required the model of the environment as shown in the following equation:\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "This equation finds out the optimal policy by finding actions that maximize the sum of rewards. However, a major caveat here is that it uses transition probabilities, which is not known in the case of model-free learning.\n",
    "\n",
    "Since we do not know the state transition probabilities p(s’,r/s,a), we can’t do a look-ahead search like DP. Hence, all the information is obtained via experience of playing the game or exploring the environment.\n",
    "\n",
    "Policy improvement is done by making the policy greedy with respect to the current value function. In this case, we have an action-value function, and therefore no model is needed to construct the greedy policy.\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "A greedy policy (like the above mentioned one) will always favor a certain action if most actions are not explored properly. There are two solutions for this:\n",
    "\n",
    "#### Monte Carlo with exploring starts\n",
    "\n",
    "All the state action pairs have non-zero probability of being the starting pair, in this algorithm. This will ensure each episode which is played will take the agent to new states and hence, there is more exploration of the environment.\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "#### Monte Carlo with epsilon-Soft\n",
    "\n",
    "What if there is a single start point for an environment (for example, a game of chess)? Exploring starts is not the right option in such cases. Recall here that in a multi-armed bandit problem, we discussed the epsilon-greedy approach.\n",
    "\n",
    "Simplest idea for ensuring continual exploration all actions are tried with non-zero probability 1 – epsilon choose the action which maximises the action value function and with probability epsilon choose an action at random.\n",
    "\n",
    "![Title](../docs/images/ARS/MethodOfFiniteDifference_Equation.png)\n",
    "\n",
    "Now that we understand the basics of Monte Carlo Control and Prediction, let’s implement the algorithm in Python. We will import the frozen lake environment from the popular OpenAI Gym toolkit.\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "## Other Repository\n",
    "https://www.analyticsvidhya.com/blog/2018/11/reinforcement-learning-introduction-monte-carlo-learning-openai-gym/?utm_source=feedburner&utm_medium=email&utm_campaign=Feed%3A%20AnalyticsVidhya%20%28Analytics%20Vidhya%29"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libraries\n",
    "import gym\n",
    "import numpy as np\n",
    "import operator\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "import random\n",
    "import itertools\n",
    "import tqdm  ## Instantly make your loops show a smart progress meter - just wrap any iterable with tqdm(iterable)\n",
    "## for i in tqdm(range(10000)):\n",
    "\n",
    "tqdm.monitor_interval = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function for Random Policy\n",
    "\n",
    "def create_random_policy(env):\n",
    "    policy = {}\n",
    "    for key in range(0, env.observation_space.n):\n",
    "        current_end = 0\n",
    "        p = {}\n",
    "        for action in range(0, env.action_space.n):\n",
    "            p[action] = 1 / env.action_space.n\n",
    "        policy[key] = p\n",
    "    return policy\n",
    "\n",
    "## Dictionary for storing the state action value\n",
    "def create_state_action_dictionary(env, policy):\n",
    "    Q = {}\n",
    "    for key in policy.keys():\n",
    "         Q[key] = {a: 0.0 for a in range(0, env.action_space.n)}\n",
    "    return Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to play episode\n",
    "\n",
    "def run_game(env, policy, display=True):\n",
    "    env.reset()\n",
    "    episode = []\n",
    "    finished = False\n",
    "\n",
    "    while not finished:\n",
    "        s = env.env.s\n",
    "        if display:\n",
    "            clear_output(True)\n",
    "            env.render()\n",
    "            sleep(1)\n",
    "\n",
    "        timestep = []\n",
    "        timestep.append(s)\n",
    "        n = random.uniform(0, sum(policy[s].values()))\n",
    "        top_range = 0\n",
    "        for prob in policy[s].items():\n",
    "            top_range += prob[1]\n",
    "            if n < top_range:\n",
    "                action = prob[0]\n",
    "                break \n",
    "        state, reward, finished, info = env.step(action)\n",
    "        timestep.append(action)\n",
    "        timestep.append(reward)\n",
    "\n",
    "        episode.append(timestep)\n",
    "\n",
    "    if display:\n",
    "        clear_output(True)\n",
    "        env.render()\n",
    "        sleep(1)\n",
    "    return episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Function to test policy and print win percentage\n",
    "def test_policy(policy, env):\n",
    "    wins = 0\n",
    "    r = 100\n",
    "    for i in range(r):\n",
    "        w = run_game(env, policy, display=False)[-1][-1]\n",
    "        if w == 1:\n",
    "            wins += 1\n",
    "    return wins / r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First Visit Monte Carlo Prediction and Control\n",
    "def monte_carlo_e_soft(env, episodes=100, policy=None, epsilon=0.01):\n",
    "    if not policy:\n",
    "        policy = create_random_policy(env)  # Create an empty dictionary to store state action values    \n",
    "    Q = create_state_action_dictionary(env, policy) # Empty dictionary for storing rewards for each state-action pair\n",
    "    returns = {} # 3.\n",
    "    \n",
    "    for _ in range(episodes): # Looping through episodes\n",
    "        G = 0 # Store cumulative reward in G (initialized at 0)\n",
    "        episode = run_game(env=env, policy=policy, display=False) # Store state, action and value respectively \n",
    "        \n",
    "        # for loop through reversed indices of episode array. \n",
    "        # The logic behind it being reversed is that the eventual reward would be at the end. \n",
    "        # So we have to go back from the last timestep to the first one propagating result from the future.\n",
    "        \n",
    "        for i in reversed(range(0, len(episode))):   \n",
    "            s_t, a_t, r_t = episode[i] \n",
    "            state_action = (s_t, a_t)\n",
    "            G += r_t # Increment total reward by reward on current timestep\n",
    "            \n",
    "            if not state_action in [(x[0], x[1]) for x in episode[0:i]]: # \n",
    "                if returns.get(state_action):\n",
    "                    returns[state_action].append(G)\n",
    "                else:\n",
    "                    returns[state_action] = [G]   \n",
    "                    \n",
    "                Q[s_t][a_t] = sum(returns[state_action]) / len(returns[state_action]) # Average reward across episodes\n",
    "                \n",
    "                Q_list = list(map(lambda x: x[1], Q[s_t].items())) # Finding the action with maximum value\n",
    "                indices = [i for i, x in enumerate(Q_list) if x == max(Q_list)]\n",
    "                max_Q = random.choice(indices)\n",
    "                \n",
    "                A_star = max_Q # 14.\n",
    "                \n",
    "                for a in policy[s_t].items(): # Update action probability for s_t in policy\n",
    "                    if a[0] == A_star:\n",
    "                        policy[s_t][a[0]] = 1 - epsilon + (epsilon / abs(sum(policy[s_t].values())))\n",
    "                    else:\n",
    "                        policy[s_t][a[0]] = (epsilon / abs(sum(policy[s_t].values())))\n",
    "\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Running the Algorithm\n",
    "env = gym.make('FrozenLake8x8-v0')\n",
    "policy = monte_carlo_e_soft(env, episodes=5000)\n",
    "test_policy(policy, env)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Normalizing the  states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProjectVirEnv",
   "language": "python",
   "name": "myprojectvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
