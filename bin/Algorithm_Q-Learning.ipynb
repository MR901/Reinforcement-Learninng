{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q-Learning\n",
    "\n",
    "Q-Learning is an off-policy, model-free RL algorithm based on the well-known Bellman Equation:\n",
    "\n",
    "![bell](../docs/images/QLearning/1_Bellman_Equation.png) \n",
    "\n",
    "E in the above equation refers to the expectation, while ƛ refers to the discount factor. We can re-write it in the form of Q-value:\n",
    "\n",
    "\n",
    "Bellman Equation In Q-value Form (https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit)\n",
    "The optimal Q-value, denoted as Q* can be expressed as:\n",
    "\n",
    "\n",
    "Optimal Q-value (https://zhuanlan.zhihu.com/p/21378532?refer=intelligentunit)\n",
    "The goal is to maximize the Q-value. Before diving into the method to optimize Q-value, I would like to discuss two value update methods that are closely related to Q-learning.\n",
    "\n",
    "Policy Iteration\n",
    "\n",
    "Policy iteration runs an loop between policy evaluation and policy improvement.\n",
    "\n",
    "\n",
    "Policy Iteration (http://blog.csdn.net/songrotek/article/details/51378582)\n",
    "Policy evaluation estimates the value function V with the greedy policy obtained from the last policy improvement. Policy improvement, on the other hand, updates the policy with the action that maximizes V for each of the state. The update equations are based on Bellman Equation. It keeps iterating till convergence.\n",
    "\n",
    "\n",
    "Pseudo Code For Policy Iteration (http://blog.csdn.net/songrotek/article/details/51378582)\n",
    "Value Iteration\n",
    "\n",
    "Value Iteration only contains one component. It updates the value function V based on the Optimal Bellman Equation.\n",
    "\n",
    "\n",
    "Optimal Bellman Equation (http://blog.csdn.net/songrotek/article/details/51378582)\n",
    "\n",
    "Pseudo Code For Value Iteration (http://blog.csdn.net/songrotek/article/details/51378582)\n",
    "After the iteration converges, the optimal policy is straight-forwardly derived by applying an argument-max function for all of the states.\n",
    "\n",
    "Note that these two methods require the knowledge of the transition probability p, indicating that it is a model-based algorithm. However, as I mentioned earlier, model-based algorithm suffers from scalability problem. So how does Q-learning solves this problem?\n",
    "\n",
    "\n",
    "Q-Learning Update Equation (https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning)\n",
    "α refers to the learning rate (i.e. how fast are we approaching the goal). The idea behind Q-learning is highly relied on value iteration. However, the update equation is replaced with the above formula. As a result, we do not need to worry about the transition probability anymore.\n",
    "\n",
    "\n",
    "Q-learning Pseudo Code (https://martin-thoma.com/images/2016/07/q-learning.png)\n",
    "Note that the next action a’ is chosen to maximize the next state’s Q-value instead of following the current policy. As a result, Q-learning belongs to the off-policy category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MyProjectVirEnv",
   "language": "python",
   "name": "myprojectvirenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
